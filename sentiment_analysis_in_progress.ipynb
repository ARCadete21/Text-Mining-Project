{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 5\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "from path import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path + 'data_updated\\\\train.csv', index_col=-1)\n",
    "test_data = pd.read_csv(path + 'data_updated\\\\test.csv', index_col=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive sentiments - 1\n",
    "# Negative sentiments - 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = train['lyrics'].copy()\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "train[train['lyrics'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# já feito no \"project\"\n",
    "\n",
    "# duplicates\n",
    "train[train['lyrics'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# já feito no \"project\"\n",
    "\n",
    "display(train[train.duplicated(['year', 'lyrics'])]) #85\n",
    "display(train[train.duplicated(['views', 'lyrics'])]) #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same year, \n",
    "# same title, \n",
    "# same lyrics\n",
    "# different artist\n",
    "\n",
    "train[train['title'] == \"Honeysuckle Rose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train.duplicated(['title', 'artist', 'features', 'lyrics'])] #0\n",
    "# train[train.duplicated(['title', 'artist', 'lyrics'])] #0\n",
    "display(train[train.duplicated(['title', 'lyrics'])]) #24 --- covers\n",
    "display(train[train.duplicated(['artist', 'lyrics'])]) #111 --- labeled versions (\"acoustic\", \"remix\", \"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strange Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['year'] < 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "    # Handle Email adresses\n",
    "    # Remove HTML tags\n",
    "    # Word normalization\n",
    "    # split into sentences\n",
    "\n",
    "# Feature Extraction - Encode Text into Numbers\n",
    "    # Vectorization\n",
    "        # Freq vectors\n",
    "        # One hot\n",
    "    # BOW -  calculate the frequency of words for each document\n",
    "        # 1. set of all words found in the document se\n",
    "        # 2. Count how many times each word appears for each document\n",
    "    # TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_remove_2(x):\n",
    "\n",
    "    '''\n",
    "    diferença entre esta e a sub_remove:\n",
    "    - tirei a parte que tirava os emojis - [^0-9A-Za-z]\n",
    "    - adicionei retirar emails e html tags\n",
    "     '''\n",
    "    \n",
    "    # Remove noise\n",
    "    x = re.sub(r\"(@[A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace newline and tab characters with spaces\n",
    "    x = re.sub(r'[\\t\\n]', ' ', x)\n",
    "\n",
    "    # Remove html tags\n",
    "    x = re.sub(re.compile('<.*?>'), '', x)\n",
    "\n",
    "    # Remove email addresses\n",
    "    x = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', x)\n",
    "\n",
    "    # Remove isolated consonants:\n",
    "    x = re.sub(r'\\b([^aeiou])\\b',' ',x)\n",
    "\n",
    "    # Remove space before punctuation\n",
    "    x = re.sub(r'(\\s)(?!\\w)','',x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_preprocesser(data, text_column, target=None):\n",
    "\n",
    "    #### deixei as stopwords\n",
    "\n",
    "    text_data = data[text_column].copy()\n",
    "    \n",
    "    functions = [lambda x: x.lower(), \n",
    "                    expand_contractions, \n",
    "                    sub_remove_2, \n",
    "                    sub_spaces\n",
    "                ]\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess symbols\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    for function in functions:\n",
    "        text_data = text_data.apply(function)\n",
    "\n",
    "    if target is not None:\n",
    "\n",
    "        regexp = RegexpTokenizer(r'\\w+|' + emoji_pattern.pattern) ### adidiona emojis como tokens\n",
    "        text_data = text_data.apply(regexp.tokenize)\n",
    "           \n",
    "    #     words = [word for tokens in text_data for word in tokens]\n",
    "    #     words_unique = list(set(words))\n",
    "    #     words_tagged = pos_tag(words_unique)\n",
    "    #     words_pos_map = {word: get_wordnet_pos(pos_tag) for word, pos_tag in words_tagged}\n",
    "\n",
    "    #     lemmatizer = WordNetLemmatizer()\n",
    "    #     text_data = [\n",
    "    #          [lemmatizer.lemmatize(word, pos=words_pos_map.get(word)) for word in sentence]\n",
    "    #          for sentence in text_data\n",
    "    #          ]\n",
    "        \n",
    "    #     stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    #     additional_functions = [lemmatize_with_mapping,\n",
    "    #                   lambda x: [item for item in x if item not in stopwords],\n",
    "    #                   lambda x: ' '.join(x)\n",
    "    #                   ]\n",
    "    \n",
    "    #     for additional_function in additional_functions:\n",
    "    #         text_data = text_data.apply(additional_function, args=(words_pos_map,) \n",
    "    #                                     if additional_function == lemmatize_with_mapping else ())\n",
    "\n",
    "    # # if target is not None:\n",
    "    #     text_data = pd.DataFrame(text_data, columns=[text_column])\n",
    "    #     text_data[target] = data[target]\n",
    "    \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_preproc  = sentiment_preprocesser(train, 'lyrics', 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for tokens in lyrics_preproc for word in tokens]\n",
    "print(words)\n",
    "words_unique = list(set(words))\n",
    "print(words_unique)\n",
    "words_tagged = pos_tag(words_unique)\n",
    "print(words_tagged)\n",
    "words_pos_map = {word: get_wordnet_pos(pos_tag) for word, pos_tag in words_tagged}\n",
    "print(words_pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of_words(BoW) Model\n",
    "#     create a dictionary of all the words used in the corpus\n",
    "#     convert each document to a vector that represents words available in the documents\n",
    "#     identify importance of words:\n",
    "#         Count Vector Model\n",
    "\n",
    "#         Term Frequency Vector Model\n",
    "\n",
    "#         Term Frequency-Inverse Document Frequency(TF-IDF) Model\n",
    "\n",
    "# creating count vectors for the dataset\n",
    "\n",
    "# Displaying Document Vectors\n",
    "\n",
    "# Removing Low-Frequency Words\n",
    "\n",
    "# Removing Stop Words\n",
    "\n",
    "# Distribution of words Across Different sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "#     Rule-Based \n",
    "#         Based on a set of manually crafted rules\n",
    "#         can't learn or adapt beyond what they were initially programmed for\n",
    "#         can't easily change them or add new rules\n",
    "#         can't think for themselves or make decisions outside of those rules\n",
    "\n",
    "#         1. Construct explicit rules and patterns\n",
    "#         2. use lexicons - dictionary-based systems that rely on lists of words or phrases with associated sentiment scores\n",
    "#             VADER\n",
    "#             TEXTBLOB\n",
    "#             AFINN Lexicon\n",
    "#             SentiWordNet\n",
    "#             Bing Liu’s lexicon\n",
    "\n",
    "\n",
    "#     Automatic\n",
    "#         ML algorithms (SVM, NN, NB, …)\n",
    "#     Hybrid\n",
    "#         Rule-Based + Automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EMOJIS\n",
    "# VADER performs very well with emojis, slangs and acronyms in sentences\n",
    "\n",
    "import re\n",
    "\n",
    "# Unicode ranges for emojis\n",
    "emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF]\", flags=re.UNICODE)\n",
    "\n",
    "for i in lyrics:\n",
    "    if emoji_pattern.search(i):\n",
    "        # print(i)\n",
    "        print(emoji_pattern.findall(i))\n",
    "        \n",
    "#####  apagar os not related com emoções e deixar os outrs???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tag\n",
    "tag = train['tag'].copy()\n",
    "\n",
    "# functions = [lambda x: x.lower(), \n",
    "#                 expand_contractions, \n",
    "#                 sub_remove, \n",
    "#                 sub_spaces]\n",
    "\n",
    "# for function in functions:\n",
    "#     text_data = text_data.apply(function)\n",
    "\n",
    "# if target is not None:\n",
    "#     regexp = RegexpTokenizer('\\w+')\n",
    "#     text_data = text_data.apply(regexp.tokenize)\n",
    "\n",
    "#     words = [word for tokens in text_data for word in tokens]\n",
    "#     words_unique = list(set(words))\n",
    "#     words_tagged = pos_tag(words_unique)\n",
    "#     words_pos_map = {word: get_wordnet_pos(pos_tag) for word, pos_tag in words_tagged}\n",
    "\n",
    "#     # lemmatizer = WordNetLemmatizer()\n",
    "#     # text_data = [\n",
    "#     #     [lemmatizer.lemmatize(word, pos=words_pos_map.get(word)) for word in sentence]\n",
    "#     #     for sentence in text_data\n",
    "#     #     ]\n",
    "    \n",
    "#     # stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#     additional_functions = [lemmatize_with_mapping,\n",
    "#                 #   lambda x: [item for item in x if item not in stopwords],\n",
    "#                     lambda x: ' '.join(x)\n",
    "#                     ]\n",
    "\n",
    "#     for additional_function in additional_functions:\n",
    "#         text_data = text_data.apply(additional_function, args=(words_pos_map,) \n",
    "#                                     if additional_function == lemmatize_with_mapping else ())\n",
    "\n",
    "# # if target is not None:\n",
    "#     text_data = pd.DataFrame(text_data, columns=[text_column])\n",
    "#     text_data[target] = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyrics_train = pd.read_csv(r'C:\\Users\\bruna\\Desktop\\data_updated\\lyrics_train.csv', index_col=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest task -> positive or negative\n",
    "# More complex -> rank the attitude of this text from 1 to 5\n",
    "# Advanced -> detect the target, source, or complex attitude type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### VADER\n",
    "# {'compound': 0.6588, 'neg': 0.0, 'neu': 0.406, 'pos': 0.594}\n",
    "# [-1] (Extremely Negative)\n",
    "# [1] (Extremely Positive)\n",
    "# [0] Neutral or Neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXTBLOB\n",
    "# `Sentiment(polarity=1.0, subjectivity=0.75)`\n",
    "\n",
    "# polarity → measures the sentiment or emotional tone of the text\n",
    "\n",
    "# - ranges between [-1, 1]\n",
    "#     - 1 indicates a highly negative sentiment\n",
    "#     - 0 indicates a neutral sentiment\n",
    "#     - 1 indicates a highly positive sentiment\n",
    "    \n",
    "\n",
    "# subjectivity → measures how objective or subjective the text is\n",
    "\n",
    "# - ranges between [0, 1]\n",
    "#     - 0 indicates a highly objective piece of text → fact-based content\n",
    "#     - 1 indicates a highly subjective (opinionated) piece of text → personal opinions, emotions, judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenges:\n",
    "#     phrase with negation\n",
    "#     negation, inverted word order - Disliking horror movies is not uncommon\n",
    "#     The adverb sometimes modifies the sentiment - ex: Somentimes\n",
    "#     sarcasm\n",
    "#     negative term used in a positive way\n",
    "#     difficult to categorize\n",
    "\n",
    "#     Objective / Subjective\n",
    "#     Context and Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context-Dependent Erros:\n",
    "#     Sarcasm\n",
    "#     Polarity\n",
    "#     Polysemy\n",
    "#     Emojis -  emojis sometimes cannot be classified accurately and thus are removed from many analysis\n",
    "#         (If those are removed from text, one ends up with a noncomprehensive analysis)\n",
    "#     gender stereotypes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
